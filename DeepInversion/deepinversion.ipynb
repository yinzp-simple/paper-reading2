{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Kernel is dead",
     "output_type": "error",
     "traceback": [
      "Error: Kernel is dead",
      "at g._sendKernelShellControl (/home/ubuntu/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1006305)",
      "at g.sendShellMessage (/home/ubuntu/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1006074)",
      "at g.requestExecute (/home/ubuntu/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:1008616)",
      "at d.requestExecute (/home/ubuntu/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:37:328037)",
      "at S.requestExecute (/home/ubuntu/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:32:19306)",
      "at w.executeCodeCell (/home/ubuntu/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300924)",
      "at w.execute (/home/ubuntu/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:300551)",
      "at w.start (/home/ubuntu/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:296215)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/ubuntu/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310950)",
      "at async t.CellExecutionQueue.start (/home/ubuntu/.vscode/extensions/ms-toolsai.jupyter-2021.9.1101343141/out/client/extension.js:52:310490)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, ImageFolder\n",
    "from torchvision.datasets.imagenet import ImageNet\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 教师模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out =F.relu(out)\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, out_feature=False):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        feature = out.view(out.size(0), -1)\n",
    "        out = self.linear(feature)\n",
    "        if out_feature == False:\n",
    "            return out\n",
    "        else:\n",
    "            return out, feature\n",
    "def ResNet18(num_classes=10):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n",
    " \n",
    "def ResNet34(num_classes=10):\n",
    "    return ResNet(BasicBlock, [3,4,6,3], num_classes)\n",
    " \n",
    "def ResNet50(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3,4,6,3], num_classes)\n",
    " \n",
    "def ResNet101(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3,4,23,3], num_classes)\n",
    " \n",
    "def ResNet152(num_classes=10):\n",
    "    return ResNet(Bottleneck, [3,8,36,3], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 教师训练类搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherTrainer:\n",
    "\tdef __init__(self, path_ckpt, path_loss, path_dataset, name_dataset='cifar10', \n",
    "\t\t\t\t bs=512, num_epochsaving=100, resume_train=False, path_resume=None):\n",
    "\t\tif name_dataset == 'cifar10':\n",
    "\t\t\ttransform_train = transforms.Compose([\n",
    "\t\t\t\ttransforms.RandomCrop(32, padding=4),\n",
    "\t\t\t\ttransforms.RandomHorizontalFlip(),\n",
    "\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\ttransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "\t\t\t\t])\n",
    "\t\t\ttransform_test = transforms.Compose([\n",
    "\t\t\t\ttransforms.ToTensor(),\n",
    "\t\t\t\ttransforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "\t\t\t])\n",
    "\t\t\tself.dataset_train = CIFAR10(path_dataset, transform=transform_train)\n",
    "\t\t\tself.dataset_test = CIFAR10(path_dataset, train=False, transform=transform_test)\n",
    "\t\t\tself.dataset_test_loader = DataLoader(self.dataset_test, batch_size=512, num_workers=0)\n",
    "\t\t\tself.dataset_train_loader = DataLoader(self.dataset_train, batch_size=bs, shuffle=True, num_workers=8)\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\ttorch.cuda.set_device('cuda:0')\n",
    "\t\t\tnet = ResNet34().cuda()\n",
    "\t\t\tself.net = nn.DataParallel(net,device_ids=[0,1,2,3], output_device=0)\n",
    "\t\t\tself.criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "\t\t\tself.optimizer = torch.optim.SGD(self.net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\t\t\tself.lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=5, T_mult=2, eta_min=0, last_epoch=-1)\n",
    "\t\t\tself.last_epoch = 0\n",
    "\t\t\tif resume_train:\n",
    "\t\t\t\tckpt = torch.load(path_resume)\n",
    "\t\t\t\tnet.load_state_dict(ckpt['net'])\n",
    "\t\t\t\tself.optimizer.load_state_dict(ckpt['optimizer'])\n",
    "\t\t\t\tself.lr_scheduler.load_state_dict(ckpt['lr_scheduler'])\n",
    "\t\t\t\tself.last_epoch = ckpt['epoch']\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t\t# 训练相关参数\n",
    "\t\tself.best_accr = 0\n",
    "\t\tself.list_loss = []\n",
    "\t\tself.path_ckpt = path_ckpt\n",
    "\t\tself.path_loss = path_loss\n",
    "\t\tself.num_epochsaving = num_epochsaving\n",
    "\tdef train(self, epochs):\n",
    "\t\t\n",
    "\t\tfor epoch in range(self.last_epoch+1, epochs+1):\n",
    "\t\t\tself.net.train()\n",
    "\t\t\tloss_epoch = 0\n",
    "\t\t\tfor i, (batch_img, batch_label) in enumerate(self.dataset_train_loader, start=1):\n",
    "\t\t\t\tbatch_img, batch_label = Variable(batch_img).cuda(non_blocking=True), Variable(batch_label).cuda(non_blocking=True)\n",
    "\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\toutput = self.net(batch_img)\n",
    "\t\t\t\tloss = self.criterion(output, batch_label)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\tself.optimizer.step()\n",
    "\t\t\t\tloss_epoch += loss.data.item()\n",
    "\t\t\t# 一个epoch结束\n",
    "\t\t\tself.lr_scheduler.step()\n",
    "\t\t\tself.list_loss.append(loss_epoch)\n",
    "\t\t\tprint('EPOCH:%d, LOSS:%f'%(epoch, loss_epoch))\n",
    "\t\t\t# 测试\n",
    "\t\t\tself.test(epoch)\n",
    "\t\tself.save_experiment(epochs)\n",
    "\t\t\n",
    "\t\n",
    "\tdef adjust_lr(self, epoch):\n",
    "\t\tif epoch < 80:\n",
    "\t\t\tlr = 0.1\n",
    "\t\tif epoch < 120:\n",
    "\t\t\tlr = 0.01\n",
    "\t\telse:\n",
    "\t\t\tlr = 0.001\n",
    "\t\tfor param_group in self.optimizer.param_groups:\n",
    "\t\t\tparam_group['lr'] = lr\n",
    "\n",
    "\tdef test(self, epoch):\n",
    "\t\tself.net.eval()\n",
    "\t\ttotal_correct = 0\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor i, (images, labels) in enumerate(self.dataset_test_loader, start=1):\n",
    "\t\t\t\timages, labels = Variable(images).cuda(), Variable(labels).cuda()\n",
    "\t\t\t\toutput = self.net(images)\n",
    "\t\t\t\tpred = output.data.max(1)[1]\n",
    "\t\t\t\ttotal_correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    "            \n",
    "\t\tacc = float(total_correct) / len(self.dataset_test)\n",
    "\t\tif acc > self.best_accr:\n",
    "\t\t\tself.best_accr = acc\n",
    "\t\t\tif epoch > self.num_epochsaving:\n",
    "\t\t\t\tself.save_model(self.path_ckpt, epoch)\n",
    "\t\t\n",
    "\t\tprint('Test Accuracy:%f' % (acc))\n",
    "\n",
    "\tdef save_model(self, path, epoch):\n",
    "\t\tstate = {\n",
    "\t\t\t'net': self.net.state_dict(), \n",
    "\t\t\t'optimizer':self.optimizer.state_dict(), \n",
    "\t\t\t'lr_scheduler':self.lr_scheduler.state_dict(),\n",
    "\t\t\t'epoch':epoch}\n",
    "\t\tfilename = path + 'teacher__accr%f_epoch%d.pth'%(self.best_accr, epoch)\n",
    "\t\ttorch.save(state, filename)\n",
    "\tdef save_experiment(self, epochs):\n",
    "\t\tlossfile = np.array(self.list_loss)\n",
    "\t\tnp.save(self.path_loss + '/teacher_loss_{}'.format(epochs), lossfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deepinversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputTrainer:\n",
    "    def __init__(self, path_ckpt_teacher, path_inputs_saving):\n",
    "        # teacher and student net\n",
    "        ckpt = torch.load(path_ckpt_teacher)\n",
    "        print('Using trained teacher network...')\n",
    "        self.teacher = resnet.ResNet34().cuda()\n",
    "        self.teacher.load_state_dict(ckpt['net'])\n",
    "        self.student = resnet.ResNet18().cuda()\n",
    "        self.teacher.eval()\n",
    "        self.student.eval()\n",
    "        \n",
    "        self.inputs = torch.randn((256, 3, 32, 32), requires_grad=True, device='cuda', dtype=torch.float)\n",
    "        self.criterion = nn.CrossEntropyLoss().cuda()\n",
    "        self.optimizer_in = torch.optim.Adam([self.inputs], lr=0.05)\n",
    "        self.optimizer_in.state = collections.defaultdict(dict)\n",
    "        #arguments\n",
    "        self.last_epoch = 0\n",
    "        self.loss_r_feature = list()\n",
    "        self.path_inputs_saving = path_inputs_saving\n",
    "        \n",
    "    def train(self, epochs, l2_coeff=0.0, var_scale=2.5e-5, bn_reg_scale=10):\n",
    "        # batch size 250+6\n",
    "        targets = torch.LongTensor([0,1,2,3,4,5,6,7,8,9]*25 + [0,1,2,3,4,5]).cuda()\n",
    "        largest_loss = 1e6\n",
    "        for epoch in range(self.last_epoch+1, epochs+1):\n",
    "            off1 = random.randint(-2,-2)\n",
    "            off2 = random.randint(-2,-2)\n",
    "            # shift the input along the second(w) and third(h) axis by a random number\n",
    "            inputs_jit = torch.roll(self.inputs, shifts=(off1, off2), dims=(2,3))\n",
    "            \n",
    "            self.optimizer_in.zero_grad()\n",
    "            self.teacher.zero_grad()\n",
    "            outputs = self.teacher(inputs_jit)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # apply total variation regularization\n",
    "            diff1 = inputs_jit[:,:,:,:-1] - inputs_jit[:,:,:,1:]\n",
    "            diff2 = inputs_jit[:,:,:-1,:] - inputs_jit[:,:,1:,:]\n",
    "            diff3 = inputs_jit[:,:,1:,:-1] - inputs_jit[:,:,:-1,1:]\n",
    "            diff4 = inputs_jit[:,:,:-1,:-1] - inputs_jit[:,:,1:,1:]\n",
    "            loss_var = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n",
    "            loss = loss + var_scale*loss_var\n",
    "\n",
    "            # 每个BN层的running_mean和running_var与新输入的差值\n",
    "            loss_distr = sum([r_feature for r_feature in self.loss_r_feature])\n",
    "            loss = loss + bn_reg_scale*loss_distr\n",
    "\n",
    "            # l2 loss\n",
    "            loss = loss + l2_coeff*torch.norm(inputs_jit, 2)\n",
    "            print(epoch, loss.item())\n",
    "            # 采用这种保存方法，节省保存次数\n",
    "            if largest_loss > loss.item():\n",
    "                largest_loss = loss.item()\n",
    "                best_inputs = self.inputs.data\n",
    "            loss.backward()\n",
    "            self.optimizer_in.step()\n",
    "        vutils.save_image(best_inputs[:20].clone(), self.path_inputs_saving, \n",
    "                          normalize=True, scale_each=True, nrow=10)\n",
    "            \n",
    "    def hook_fn(self, module, input_data, output_data):\n",
    "        num_ch = input_data[0].shape[1]\n",
    "        mean = input_data[0].mean([0,2,3])\n",
    "        var = input_data[0].permute(1,0,2,3).contiguous().view([num_ch, -1]).var(1, unbiased=False)\n",
    "        r_feature = torch.norm(module.running_var.data.type(var.type()) - var, 2) + torch.norm(module.running_mean.data.type(mean.type()) - mean, 2)\n",
    "        self.loss_r_feature.append(r_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练教师网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:1, LOSS:68.896122\n",
      "Test Accuracy:0.113400\n",
      "EPOCH:2, LOSS:48.271791\n",
      "Test Accuracy:0.289700\n",
      "EPOCH:3, LOSS:44.163279\n",
      "Test Accuracy:0.356200\n",
      "EPOCH:4, LOSS:41.442758\n",
      "Test Accuracy:0.396000\n",
      "EPOCH:5, LOSS:39.679080\n",
      "Test Accuracy:0.422000\n",
      "EPOCH:6, LOSS:48.267901\n",
      "Test Accuracy:0.304400\n",
      "EPOCH:7, LOSS:42.839829\n",
      "Test Accuracy:0.379700\n",
      "EPOCH:8, LOSS:39.221532\n",
      "Test Accuracy:0.357800\n",
      "EPOCH:9, LOSS:36.694259\n",
      "Test Accuracy:0.439000\n",
      "EPOCH:10, LOSS:34.455036\n",
      "Test Accuracy:0.467500\n",
      "EPOCH:11, LOSS:32.197157\n",
      "Test Accuracy:0.515300\n",
      "EPOCH:12, LOSS:30.074572\n",
      "Test Accuracy:0.577400\n",
      "EPOCH:13, LOSS:28.109925\n",
      "Test Accuracy:0.584400\n",
      "EPOCH:14, LOSS:26.505617\n",
      "Test Accuracy:0.628200\n",
      "EPOCH:15, LOSS:25.531642\n",
      "Test Accuracy:0.633900\n",
      "EPOCH:16, LOSS:46.982348\n",
      "Test Accuracy:0.345700\n",
      "EPOCH:17, LOSS:41.582078\n",
      "Test Accuracy:0.399900\n",
      "EPOCH:18, LOSS:38.344050\n",
      "Test Accuracy:0.450700\n",
      "EPOCH:19, LOSS:35.624523\n",
      "Test Accuracy:0.440200\n",
      "EPOCH:20, LOSS:33.561480\n",
      "Test Accuracy:0.502700\n",
      "EPOCH:21, LOSS:31.083786\n",
      "Test Accuracy:0.553700\n",
      "EPOCH:22, LOSS:28.406961\n",
      "Test Accuracy:0.579500\n",
      "EPOCH:23, LOSS:26.338855\n",
      "Test Accuracy:0.596300\n",
      "EPOCH:24, LOSS:24.535351\n",
      "Test Accuracy:0.623900\n",
      "EPOCH:25, LOSS:22.470811\n",
      "Test Accuracy:0.662700\n",
      "EPOCH:26, LOSS:21.078813\n",
      "Test Accuracy:0.661400\n",
      "EPOCH:27, LOSS:19.794859\n",
      "Test Accuracy:0.673600\n",
      "EPOCH:28, LOSS:18.335070\n",
      "Test Accuracy:0.712700\n",
      "EPOCH:29, LOSS:17.238837\n",
      "Test Accuracy:0.738800\n",
      "EPOCH:30, LOSS:16.020003\n",
      "Test Accuracy:0.754600\n",
      "EPOCH:31, LOSS:15.252952\n",
      "Test Accuracy:0.765800\n",
      "EPOCH:32, LOSS:14.460282\n",
      "Test Accuracy:0.780300\n",
      "EPOCH:33, LOSS:13.834495\n",
      "Test Accuracy:0.785300\n",
      "EPOCH:34, LOSS:13.376195\n",
      "Test Accuracy:0.788100\n",
      "EPOCH:35, LOSS:13.138388\n",
      "Test Accuracy:0.789200\n",
      "EPOCH:36, LOSS:34.793628\n",
      "Test Accuracy:0.497600\n",
      "EPOCH:37, LOSS:28.207067\n",
      "Test Accuracy:0.558100\n",
      "EPOCH:38, LOSS:23.685784\n",
      "Test Accuracy:0.640600\n",
      "EPOCH:39, LOSS:20.852101\n",
      "Test Accuracy:0.671600\n",
      "EPOCH:40, LOSS:18.763413\n",
      "Test Accuracy:0.665300\n",
      "EPOCH:41, LOSS:17.544026\n",
      "Test Accuracy:0.660300\n",
      "EPOCH:42, LOSS:16.506373\n",
      "Test Accuracy:0.736300\n",
      "EPOCH:43, LOSS:14.918881\n",
      "Test Accuracy:0.752000\n",
      "EPOCH:44, LOSS:13.939221\n",
      "Test Accuracy:0.771300\n",
      "EPOCH:45, LOSS:13.238318\n",
      "Test Accuracy:0.781400\n",
      "EPOCH:46, LOSS:12.304043\n",
      "Test Accuracy:0.802900\n",
      "EPOCH:47, LOSS:11.236701\n",
      "Test Accuracy:0.798000\n",
      "EPOCH:48, LOSS:10.841448\n",
      "Test Accuracy:0.778600\n",
      "EPOCH:49, LOSS:10.247449\n",
      "Test Accuracy:0.827900\n",
      "EPOCH:50, LOSS:9.446774\n",
      "Test Accuracy:0.821000\n",
      "EPOCH:51, LOSS:8.850296\n",
      "Test Accuracy:0.840100\n",
      "EPOCH:52, LOSS:8.249587\n",
      "Test Accuracy:0.837400\n",
      "EPOCH:53, LOSS:7.866475\n",
      "Test Accuracy:0.837900\n",
      "EPOCH:54, LOSS:7.341451\n",
      "Test Accuracy:0.854500\n",
      "EPOCH:55, LOSS:6.819573\n",
      "Test Accuracy:0.856000\n",
      "EPOCH:56, LOSS:6.419066\n",
      "Test Accuracy:0.842500\n",
      "EPOCH:57, LOSS:6.013425\n",
      "Test Accuracy:0.850100\n",
      "EPOCH:58, LOSS:5.626780\n",
      "Test Accuracy:0.848100\n",
      "EPOCH:59, LOSS:5.297189\n",
      "Test Accuracy:0.861500\n",
      "EPOCH:60, LOSS:4.766037\n",
      "Test Accuracy:0.882400\n",
      "EPOCH:61, LOSS:4.350494\n",
      "Test Accuracy:0.880400\n",
      "EPOCH:62, LOSS:3.953116\n",
      "Test Accuracy:0.878000\n",
      "EPOCH:63, LOSS:3.455523\n",
      "Test Accuracy:0.880500\n",
      "EPOCH:64, LOSS:3.236690\n",
      "Test Accuracy:0.887300\n",
      "EPOCH:65, LOSS:2.811036\n",
      "Test Accuracy:0.888000\n",
      "EPOCH:66, LOSS:2.571357\n",
      "Test Accuracy:0.889700\n",
      "EPOCH:67, LOSS:2.399244\n",
      "Test Accuracy:0.887800\n",
      "EPOCH:68, LOSS:2.054609\n",
      "Test Accuracy:0.896500\n",
      "EPOCH:69, LOSS:1.886069\n",
      "Test Accuracy:0.895300\n",
      "EPOCH:70, LOSS:1.711084\n",
      "Test Accuracy:0.896200\n",
      "EPOCH:71, LOSS:1.589464\n",
      "Test Accuracy:0.898200\n",
      "EPOCH:72, LOSS:1.492710\n",
      "Test Accuracy:0.898100\n",
      "EPOCH:73, LOSS:1.447683\n",
      "Test Accuracy:0.897200\n",
      "EPOCH:74, LOSS:1.404808\n",
      "Test Accuracy:0.898500\n",
      "EPOCH:75, LOSS:1.386800\n",
      "Test Accuracy:0.897000\n",
      "EPOCH:76, LOSS:8.953149\n",
      "Test Accuracy:0.764700\n",
      "EPOCH:77, LOSS:8.215385\n",
      "Test Accuracy:0.822100\n",
      "EPOCH:78, LOSS:7.098617\n",
      "Test Accuracy:0.838500\n",
      "EPOCH:79, LOSS:6.337893\n",
      "Test Accuracy:0.840200\n",
      "EPOCH:80, LOSS:6.339321\n",
      "Test Accuracy:0.822500\n",
      "EPOCH:81, LOSS:5.683834\n",
      "Test Accuracy:0.863300\n",
      "EPOCH:82, LOSS:5.248608\n",
      "Test Accuracy:0.806200\n",
      "EPOCH:83, LOSS:5.025498\n",
      "Test Accuracy:0.847000\n",
      "EPOCH:84, LOSS:4.770761\n",
      "Test Accuracy:0.863800\n",
      "EPOCH:85, LOSS:4.785182\n",
      "Test Accuracy:0.850800\n",
      "EPOCH:86, LOSS:4.311231\n",
      "Test Accuracy:0.847300\n",
      "EPOCH:87, LOSS:3.869972\n",
      "Test Accuracy:0.859100\n",
      "EPOCH:88, LOSS:3.636458\n",
      "Test Accuracy:0.862200\n",
      "EPOCH:89, LOSS:3.582106\n",
      "Test Accuracy:0.856900\n",
      "EPOCH:90, LOSS:3.515722\n",
      "Test Accuracy:0.860700\n",
      "EPOCH:91, LOSS:3.357829\n",
      "Test Accuracy:0.879200\n",
      "EPOCH:92, LOSS:3.066010\n",
      "Test Accuracy:0.881800\n",
      "EPOCH:93, LOSS:2.890101\n",
      "Test Accuracy:0.887200\n",
      "EPOCH:94, LOSS:2.845592\n",
      "Test Accuracy:0.883600\n",
      "EPOCH:95, LOSS:2.639308\n",
      "Test Accuracy:0.872700\n",
      "EPOCH:96, LOSS:2.427368\n",
      "Test Accuracy:0.856000\n",
      "EPOCH:97, LOSS:2.187189\n",
      "Test Accuracy:0.878000\n",
      "EPOCH:98, LOSS:2.179725\n",
      "Test Accuracy:0.889400\n",
      "EPOCH:99, LOSS:2.167747\n",
      "Test Accuracy:0.882800\n",
      "EPOCH:100, LOSS:1.946100\n",
      "Test Accuracy:0.890100\n",
      "EPOCH:101, LOSS:1.875927\n",
      "Test Accuracy:0.883900\n",
      "EPOCH:102, LOSS:1.720294\n",
      "Test Accuracy:0.883100\n",
      "EPOCH:103, LOSS:1.562452\n",
      "Test Accuracy:0.891100\n",
      "EPOCH:104, LOSS:1.467634\n",
      "Test Accuracy:0.894500\n",
      "EPOCH:105, LOSS:1.433654\n",
      "Test Accuracy:0.894100\n",
      "EPOCH:106, LOSS:1.349009\n",
      "Test Accuracy:0.872900\n",
      "EPOCH:107, LOSS:1.225699\n",
      "Test Accuracy:0.889000\n",
      "EPOCH:108, LOSS:1.204657\n",
      "Test Accuracy:0.900000\n",
      "EPOCH:109, LOSS:1.017367\n",
      "Test Accuracy:0.902400\n",
      "EPOCH:110, LOSS:0.953211\n",
      "Test Accuracy:0.899900\n",
      "EPOCH:111, LOSS:0.823162\n",
      "Test Accuracy:0.891600\n",
      "EPOCH:112, LOSS:0.778460\n",
      "Test Accuracy:0.895300\n",
      "EPOCH:113, LOSS:0.835483\n",
      "Test Accuracy:0.904000\n",
      "EPOCH:114, LOSS:0.704360\n",
      "Test Accuracy:0.902500\n",
      "EPOCH:115, LOSS:0.526021\n",
      "Test Accuracy:0.908600\n",
      "EPOCH:116, LOSS:0.458600\n",
      "Test Accuracy:0.909100\n",
      "EPOCH:117, LOSS:0.438422\n",
      "Test Accuracy:0.910500\n",
      "EPOCH:118, LOSS:0.384837\n",
      "Test Accuracy:0.910900\n",
      "EPOCH:119, LOSS:0.283670\n",
      "Test Accuracy:0.911200\n",
      "EPOCH:120, LOSS:0.241760\n",
      "Test Accuracy:0.912500\n",
      "EPOCH:121, LOSS:0.200405\n",
      "Test Accuracy:0.912300\n",
      "EPOCH:122, LOSS:0.199021\n",
      "Test Accuracy:0.916700\n",
      "EPOCH:123, LOSS:0.188146\n",
      "Test Accuracy:0.915400\n",
      "EPOCH:124, LOSS:0.165277\n",
      "Test Accuracy:0.914000\n",
      "EPOCH:125, LOSS:0.128688\n",
      "Test Accuracy:0.914700\n",
      "EPOCH:126, LOSS:0.107519\n",
      "Test Accuracy:0.915400\n",
      "EPOCH:127, LOSS:0.100187\n",
      "Test Accuracy:0.919200\n",
      "EPOCH:128, LOSS:0.087919\n",
      "Test Accuracy:0.918300\n",
      "EPOCH:129, LOSS:0.083577\n",
      "Test Accuracy:0.920600\n",
      "EPOCH:130, LOSS:0.073465\n",
      "Test Accuracy:0.919200\n",
      "EPOCH:131, LOSS:0.062825\n",
      "Test Accuracy:0.920200\n",
      "EPOCH:132, LOSS:0.059082\n",
      "Test Accuracy:0.921000\n",
      "EPOCH:133, LOSS:0.050771\n",
      "Test Accuracy:0.922200\n",
      "EPOCH:134, LOSS:0.053576\n",
      "Test Accuracy:0.921800\n",
      "EPOCH:135, LOSS:0.047146\n",
      "Test Accuracy:0.923300\n",
      "EPOCH:136, LOSS:0.040807\n",
      "Test Accuracy:0.922100\n",
      "EPOCH:137, LOSS:0.038415\n",
      "Test Accuracy:0.923100\n",
      "EPOCH:138, LOSS:0.033217\n",
      "Test Accuracy:0.922500\n",
      "EPOCH:139, LOSS:0.034713\n",
      "Test Accuracy:0.922200\n",
      "EPOCH:140, LOSS:0.033168\n",
      "Test Accuracy:0.922400\n",
      "EPOCH:141, LOSS:0.032461\n",
      "Test Accuracy:0.921500\n",
      "EPOCH:142, LOSS:0.038890\n",
      "Test Accuracy:0.922700\n",
      "EPOCH:143, LOSS:0.028512\n",
      "Test Accuracy:0.922600\n",
      "EPOCH:144, LOSS:0.030912\n",
      "Test Accuracy:0.921900\n",
      "EPOCH:145, LOSS:0.030117\n",
      "Test Accuracy:0.923700\n",
      "EPOCH:146, LOSS:0.028700\n",
      "Test Accuracy:0.922700\n",
      "EPOCH:147, LOSS:0.030874\n",
      "Test Accuracy:0.923200\n",
      "EPOCH:148, LOSS:0.032574\n",
      "Test Accuracy:0.922900\n",
      "EPOCH:149, LOSS:0.030257\n",
      "Test Accuracy:0.922700\n",
      "EPOCH:150, LOSS:0.024883\n",
      "Test Accuracy:0.922800\n",
      "EPOCH:151, LOSS:0.026463\n",
      "Test Accuracy:0.923200\n",
      "EPOCH:152, LOSS:0.023724\n",
      "Test Accuracy:0.923000\n",
      "EPOCH:153, LOSS:0.024698\n",
      "Test Accuracy:0.923100\n",
      "EPOCH:154, LOSS:0.025996\n",
      "Test Accuracy:0.923100\n",
      "EPOCH:155, LOSS:0.026215\n",
      "Test Accuracy:0.922400\n",
      "EPOCH:156, LOSS:0.176199\n",
      "Test Accuracy:0.886800\n",
      "EPOCH:157, LOSS:3.314218\n",
      "Test Accuracy:0.803600\n",
      "EPOCH:158, LOSS:4.767839\n",
      "Test Accuracy:0.857300\n",
      "EPOCH:159, LOSS:3.041952\n",
      "Test Accuracy:0.881700\n",
      "EPOCH:160, LOSS:2.632314\n",
      "Test Accuracy:0.871900\n",
      "EPOCH:161, LOSS:2.433308\n",
      "Test Accuracy:0.881300\n",
      "EPOCH:162, LOSS:2.506397\n",
      "Test Accuracy:0.874300\n",
      "EPOCH:163, LOSS:2.288591\n",
      "Test Accuracy:0.874300\n",
      "EPOCH:164, LOSS:2.237223\n",
      "Test Accuracy:0.881300\n",
      "EPOCH:165, LOSS:2.089373\n",
      "Test Accuracy:0.867100\n",
      "EPOCH:166, LOSS:1.898665\n",
      "Test Accuracy:0.874100\n",
      "EPOCH:167, LOSS:1.867897\n",
      "Test Accuracy:0.882300\n",
      "EPOCH:168, LOSS:1.843842\n",
      "Test Accuracy:0.881400\n",
      "EPOCH:169, LOSS:1.902285\n",
      "Test Accuracy:0.881400\n",
      "EPOCH:170, LOSS:1.853607\n",
      "Test Accuracy:0.881700\n",
      "EPOCH:171, LOSS:1.771336\n",
      "Test Accuracy:0.886300\n",
      "EPOCH:172, LOSS:1.494588\n",
      "Test Accuracy:0.876200\n",
      "EPOCH:173, LOSS:1.402988\n",
      "Test Accuracy:0.883800\n",
      "EPOCH:174, LOSS:1.416115\n",
      "Test Accuracy:0.895000\n",
      "EPOCH:175, LOSS:1.327971\n",
      "Test Accuracy:0.872200\n",
      "EPOCH:176, LOSS:1.409592\n",
      "Test Accuracy:0.890800\n",
      "EPOCH:177, LOSS:1.294722\n",
      "Test Accuracy:0.880300\n",
      "EPOCH:178, LOSS:1.174467\n",
      "Test Accuracy:0.888400\n",
      "EPOCH:179, LOSS:1.300972\n",
      "Test Accuracy:0.885300\n",
      "EPOCH:180, LOSS:1.120209\n",
      "Test Accuracy:0.897300\n",
      "EPOCH:181, LOSS:1.126830\n",
      "Test Accuracy:0.898600\n",
      "EPOCH:182, LOSS:1.058695\n",
      "Test Accuracy:0.897200\n",
      "EPOCH:183, LOSS:1.008328\n",
      "Test Accuracy:0.894200\n",
      "EPOCH:184, LOSS:1.078046\n",
      "Test Accuracy:0.889800\n",
      "EPOCH:185, LOSS:1.225292\n",
      "Test Accuracy:0.899000\n",
      "EPOCH:186, LOSS:1.020019\n",
      "Test Accuracy:0.884200\n",
      "EPOCH:187, LOSS:0.840240\n",
      "Test Accuracy:0.904200\n",
      "EPOCH:188, LOSS:0.900698\n",
      "Test Accuracy:0.901200\n",
      "EPOCH:189, LOSS:0.836961\n",
      "Test Accuracy:0.900600\n",
      "EPOCH:190, LOSS:0.759333\n",
      "Test Accuracy:0.894400\n",
      "EPOCH:191, LOSS:0.983733\n",
      "Test Accuracy:0.903300\n",
      "EPOCH:192, LOSS:0.905315\n",
      "Test Accuracy:0.889300\n",
      "EPOCH:193, LOSS:0.759113\n",
      "Test Accuracy:0.898200\n",
      "EPOCH:194, LOSS:0.818495\n",
      "Test Accuracy:0.904300\n",
      "EPOCH:195, LOSS:0.906681\n",
      "Test Accuracy:0.901900\n",
      "EPOCH:196, LOSS:0.769273\n",
      "Test Accuracy:0.908700\n",
      "EPOCH:197, LOSS:0.777187\n",
      "Test Accuracy:0.897400\n",
      "EPOCH:198, LOSS:0.791648\n",
      "Test Accuracy:0.895300\n",
      "EPOCH:199, LOSS:0.733927\n",
      "Test Accuracy:0.899700\n",
      "EPOCH:200, LOSS:0.756365\n",
      "Test Accuracy:0.903900\n",
      "EPOCH:201, LOSS:0.755530\n",
      "Test Accuracy:0.900100\n",
      "EPOCH:202, LOSS:0.803035\n",
      "Test Accuracy:0.902800\n",
      "EPOCH:203, LOSS:0.679872\n",
      "Test Accuracy:0.905500\n",
      "EPOCH:204, LOSS:0.596783\n",
      "Test Accuracy:0.905500\n",
      "EPOCH:205, LOSS:0.550574\n",
      "Test Accuracy:0.905600\n",
      "EPOCH:206, LOSS:0.610156\n",
      "Test Accuracy:0.898200\n",
      "EPOCH:207, LOSS:0.572708\n",
      "Test Accuracy:0.901600\n",
      "EPOCH:208, LOSS:0.555492\n",
      "Test Accuracy:0.899600\n",
      "EPOCH:209, LOSS:0.651192\n",
      "Test Accuracy:0.912100\n",
      "EPOCH:210, LOSS:0.580851\n",
      "Test Accuracy:0.912700\n",
      "EPOCH:211, LOSS:0.437119\n",
      "Test Accuracy:0.912900\n",
      "EPOCH:212, LOSS:0.439429\n",
      "Test Accuracy:0.907800\n",
      "EPOCH:213, LOSS:0.511055\n",
      "Test Accuracy:0.911800\n",
      "EPOCH:214, LOSS:0.529259\n",
      "Test Accuracy:0.900100\n",
      "EPOCH:215, LOSS:0.521381\n",
      "Test Accuracy:0.904500\n",
      "EPOCH:216, LOSS:0.422307\n",
      "Test Accuracy:0.907100\n",
      "EPOCH:217, LOSS:0.487288\n",
      "Test Accuracy:0.903200\n",
      "EPOCH:218, LOSS:0.547478\n",
      "Test Accuracy:0.903000\n",
      "EPOCH:219, LOSS:0.530727\n",
      "Test Accuracy:0.908200\n",
      "EPOCH:220, LOSS:0.456315\n",
      "Test Accuracy:0.910000\n",
      "EPOCH:221, LOSS:0.424152\n",
      "Test Accuracy:0.901800\n",
      "EPOCH:222, LOSS:0.418717\n",
      "Test Accuracy:0.911400\n",
      "EPOCH:223, LOSS:0.313090\n",
      "Test Accuracy:0.915900\n",
      "EPOCH:224, LOSS:0.292649\n",
      "Test Accuracy:0.918000\n",
      "EPOCH:225, LOSS:0.313253\n",
      "Test Accuracy:0.914100\n",
      "EPOCH:226, LOSS:0.336219\n",
      "Test Accuracy:0.906000\n",
      "EPOCH:227, LOSS:0.309371\n",
      "Test Accuracy:0.910600\n",
      "EPOCH:228, LOSS:0.256558\n",
      "Test Accuracy:0.913900\n",
      "EPOCH:229, LOSS:0.227102\n",
      "Test Accuracy:0.919600\n",
      "EPOCH:230, LOSS:0.198718\n",
      "Test Accuracy:0.914600\n",
      "EPOCH:231, LOSS:0.208284\n",
      "Test Accuracy:0.918000\n",
      "EPOCH:232, LOSS:0.178980\n",
      "Test Accuracy:0.921100\n",
      "EPOCH:233, LOSS:0.193427\n",
      "Test Accuracy:0.918600\n",
      "EPOCH:234, LOSS:0.209920\n",
      "Test Accuracy:0.915800\n",
      "EPOCH:235, LOSS:0.224153\n",
      "Test Accuracy:0.918600\n",
      "EPOCH:236, LOSS:0.268519\n",
      "Test Accuracy:0.904800\n",
      "EPOCH:237, LOSS:0.241091\n",
      "Test Accuracy:0.911900\n",
      "EPOCH:238, LOSS:0.241917\n",
      "Test Accuracy:0.913800\n",
      "EPOCH:239, LOSS:0.258765\n",
      "Test Accuracy:0.909800\n",
      "EPOCH:240, LOSS:0.193909\n",
      "Test Accuracy:0.917500\n",
      "EPOCH:241, LOSS:0.142622\n",
      "Test Accuracy:0.922200\n",
      "EPOCH:242, LOSS:0.141534\n",
      "Test Accuracy:0.915900\n",
      "EPOCH:243, LOSS:0.136794\n",
      "Test Accuracy:0.920400\n",
      "EPOCH:244, LOSS:0.120656\n",
      "Test Accuracy:0.922500\n",
      "EPOCH:245, LOSS:0.113396\n",
      "Test Accuracy:0.920000\n",
      "EPOCH:246, LOSS:0.092951\n",
      "Test Accuracy:0.922100\n",
      "EPOCH:247, LOSS:0.077870\n",
      "Test Accuracy:0.927400\n",
      "EPOCH:248, LOSS:0.079691\n",
      "Test Accuracy:0.922100\n",
      "EPOCH:249, LOSS:0.086657\n",
      "Test Accuracy:0.925400\n",
      "EPOCH:250, LOSS:0.065880\n",
      "Test Accuracy:0.926000\n",
      "EPOCH:251, LOSS:0.053686\n",
      "Test Accuracy:0.925000\n",
      "EPOCH:252, LOSS:0.053075\n",
      "Test Accuracy:0.929400\n",
      "EPOCH:253, LOSS:0.047205\n",
      "Test Accuracy:0.926300\n",
      "EPOCH:254, LOSS:0.046362\n",
      "Test Accuracy:0.927300\n",
      "EPOCH:255, LOSS:0.042135\n",
      "Test Accuracy:0.930900\n",
      "EPOCH:256, LOSS:0.033345\n",
      "Test Accuracy:0.928300\n",
      "EPOCH:257, LOSS:0.046411\n",
      "Test Accuracy:0.924500\n",
      "EPOCH:258, LOSS:0.057653\n",
      "Test Accuracy:0.926300\n",
      "EPOCH:259, LOSS:0.042918\n",
      "Test Accuracy:0.931300\n",
      "EPOCH:260, LOSS:0.033842\n",
      "Test Accuracy:0.930800\n",
      "EPOCH:261, LOSS:0.026723\n",
      "Test Accuracy:0.929600\n",
      "EPOCH:262, LOSS:0.026955\n",
      "Test Accuracy:0.931300\n",
      "EPOCH:263, LOSS:0.023084\n",
      "Test Accuracy:0.932200\n",
      "EPOCH:264, LOSS:0.020659\n",
      "Test Accuracy:0.932800\n",
      "EPOCH:265, LOSS:0.021190\n",
      "Test Accuracy:0.933700\n",
      "EPOCH:266, LOSS:0.019825\n",
      "Test Accuracy:0.933500\n",
      "EPOCH:267, LOSS:0.020452\n",
      "Test Accuracy:0.932000\n",
      "EPOCH:268, LOSS:0.018095\n",
      "Test Accuracy:0.932200\n",
      "EPOCH:269, LOSS:0.018699\n",
      "Test Accuracy:0.932600\n",
      "EPOCH:270, LOSS:0.021234\n",
      "Test Accuracy:0.933200\n",
      "EPOCH:271, LOSS:0.020003\n",
      "Test Accuracy:0.932500\n",
      "EPOCH:272, LOSS:0.018589\n",
      "Test Accuracy:0.932800\n",
      "EPOCH:273, LOSS:0.016719\n",
      "Test Accuracy:0.933800\n",
      "EPOCH:274, LOSS:0.015704\n",
      "Test Accuracy:0.934200\n",
      "EPOCH:275, LOSS:0.018251\n",
      "Test Accuracy:0.934300\n",
      "EPOCH:276, LOSS:0.017444\n",
      "Test Accuracy:0.932900\n",
      "EPOCH:277, LOSS:0.017760\n",
      "Test Accuracy:0.934700\n",
      "EPOCH:278, LOSS:0.018009\n",
      "Test Accuracy:0.933600\n",
      "EPOCH:279, LOSS:0.018279\n",
      "Test Accuracy:0.932900\n",
      "EPOCH:280, LOSS:0.017510\n",
      "Test Accuracy:0.933500\n",
      "EPOCH:281, LOSS:0.016355\n",
      "Test Accuracy:0.934300\n",
      "EPOCH:282, LOSS:0.018363\n",
      "Test Accuracy:0.934500\n",
      "EPOCH:283, LOSS:0.016526\n",
      "Test Accuracy:0.934400\n",
      "EPOCH:284, LOSS:0.017043\n",
      "Test Accuracy:0.934400\n",
      "EPOCH:285, LOSS:0.019622\n",
      "Test Accuracy:0.934700\n",
      "EPOCH:286, LOSS:0.015950\n",
      "Test Accuracy:0.933800\n",
      "EPOCH:287, LOSS:0.017256\n",
      "Test Accuracy:0.934500\n",
      "EPOCH:288, LOSS:0.016697\n",
      "Test Accuracy:0.934900\n",
      "EPOCH:289, LOSS:0.016452\n",
      "Test Accuracy:0.935100\n",
      "EPOCH:290, LOSS:0.016318\n",
      "Test Accuracy:0.934700\n",
      "EPOCH:291, LOSS:0.015876\n",
      "Test Accuracy:0.935000\n",
      "EPOCH:292, LOSS:0.016147\n",
      "Test Accuracy:0.934700\n",
      "EPOCH:293, LOSS:0.016353\n",
      "Test Accuracy:0.934300\n",
      "EPOCH:294, LOSS:0.016873\n",
      "Test Accuracy:0.935200\n",
      "EPOCH:295, LOSS:0.015898\n",
      "Test Accuracy:0.934800\n",
      "EPOCH:296, LOSS:0.016032\n",
      "Test Accuracy:0.934800\n",
      "EPOCH:297, LOSS:0.016647\n",
      "Test Accuracy:0.935400\n",
      "EPOCH:298, LOSS:0.016238\n",
      "Test Accuracy:0.935400\n",
      "EPOCH:299, LOSS:0.016308\n",
      "Test Accuracy:0.935300\n",
      "EPOCH:300, LOSS:0.017177\n",
      "Test Accuracy:0.935300\n",
      "EPOCH:301, LOSS:0.016358\n",
      "Test Accuracy:0.935200\n",
      "EPOCH:302, LOSS:0.016601\n",
      "Test Accuracy:0.934900\n",
      "EPOCH:303, LOSS:0.018123\n",
      "Test Accuracy:0.935600\n",
      "EPOCH:304, LOSS:0.015316\n",
      "Test Accuracy:0.934700\n",
      "EPOCH:305, LOSS:0.018152\n",
      "Test Accuracy:0.935600\n",
      "EPOCH:306, LOSS:0.016628\n",
      "Test Accuracy:0.935400\n",
      "EPOCH:307, LOSS:0.016043\n",
      "Test Accuracy:0.936300\n",
      "EPOCH:308, LOSS:0.016189\n",
      "Test Accuracy:0.936100\n",
      "EPOCH:309, LOSS:0.017094\n",
      "Test Accuracy:0.935500\n",
      "EPOCH:310, LOSS:0.016636\n",
      "Test Accuracy:0.935900\n",
      "EPOCH:311, LOSS:0.017191\n",
      "Test Accuracy:0.935500\n",
      "EPOCH:312, LOSS:0.016788\n",
      "Test Accuracy:0.936400\n",
      "EPOCH:313, LOSS:0.017738\n",
      "Test Accuracy:0.934600\n",
      "EPOCH:314, LOSS:0.016365\n",
      "Test Accuracy:0.935200\n",
      "EPOCH:315, LOSS:0.016851\n",
      "Test Accuracy:0.935700\n",
      "EPOCH:316, LOSS:0.089502\n",
      "Test Accuracy:0.894100\n",
      "EPOCH:317, LOSS:2.913614\n",
      "Test Accuracy:0.753800\n",
      "EPOCH:318, LOSS:5.272389\n",
      "Test Accuracy:0.808800\n",
      "EPOCH:319, LOSS:4.083790\n",
      "Test Accuracy:0.844800\n",
      "EPOCH:320, LOSS:3.442459\n",
      "Test Accuracy:0.866700\n",
      "EPOCH:321, LOSS:2.473479\n",
      "Test Accuracy:0.886600\n",
      "EPOCH:322, LOSS:2.348723\n",
      "Test Accuracy:0.855100\n",
      "EPOCH:323, LOSS:2.160918\n",
      "Test Accuracy:0.876000\n",
      "EPOCH:324, LOSS:1.917189\n",
      "Test Accuracy:0.890900\n",
      "EPOCH:325, LOSS:1.864159\n",
      "Test Accuracy:0.877500\n",
      "EPOCH:326, LOSS:1.804618\n",
      "Test Accuracy:0.868600\n",
      "EPOCH:327, LOSS:1.565364\n",
      "Test Accuracy:0.870900\n",
      "EPOCH:328, LOSS:1.772761\n",
      "Test Accuracy:0.865600\n",
      "EPOCH:329, LOSS:1.649461\n",
      "Test Accuracy:0.880900\n",
      "EPOCH:330, LOSS:1.449603\n",
      "Test Accuracy:0.894200\n",
      "EPOCH:331, LOSS:1.295863\n",
      "Test Accuracy:0.890100\n",
      "EPOCH:332, LOSS:1.459431\n",
      "Test Accuracy:0.890800\n",
      "EPOCH:333, LOSS:1.152579\n",
      "Test Accuracy:0.907000\n",
      "EPOCH:334, LOSS:1.275595\n",
      "Test Accuracy:0.903900\n",
      "EPOCH:335, LOSS:1.112092\n",
      "Test Accuracy:0.875600\n",
      "EPOCH:336, LOSS:1.180901\n",
      "Test Accuracy:0.893300\n",
      "EPOCH:337, LOSS:1.132276\n",
      "Test Accuracy:0.902500\n",
      "EPOCH:338, LOSS:1.089311\n",
      "Test Accuracy:0.891400\n",
      "EPOCH:339, LOSS:1.295338\n",
      "Test Accuracy:0.893400\n",
      "EPOCH:340, LOSS:1.097621\n",
      "Test Accuracy:0.895500\n",
      "EPOCH:341, LOSS:1.088301\n",
      "Test Accuracy:0.906400\n",
      "EPOCH:342, LOSS:0.894914\n",
      "Test Accuracy:0.903500\n",
      "EPOCH:343, LOSS:0.933921\n",
      "Test Accuracy:0.893000\n",
      "EPOCH:344, LOSS:0.969159\n",
      "Test Accuracy:0.874900\n",
      "EPOCH:345, LOSS:1.138623\n",
      "Test Accuracy:0.893200\n",
      "EPOCH:346, LOSS:1.179731\n",
      "Test Accuracy:0.895300\n",
      "EPOCH:347, LOSS:0.970145\n",
      "Test Accuracy:0.896500\n",
      "EPOCH:348, LOSS:0.901880\n",
      "Test Accuracy:0.897600\n",
      "EPOCH:349, LOSS:0.938948\n",
      "Test Accuracy:0.906200\n",
      "EPOCH:350, LOSS:0.861058\n",
      "Test Accuracy:0.907200\n",
      "EPOCH:351, LOSS:0.888886\n",
      "Test Accuracy:0.902700\n",
      "EPOCH:352, LOSS:0.927459\n",
      "Test Accuracy:0.880700\n",
      "EPOCH:353, LOSS:0.939503\n",
      "Test Accuracy:0.906300\n",
      "EPOCH:354, LOSS:0.964771\n",
      "Test Accuracy:0.890200\n",
      "EPOCH:355, LOSS:0.939940\n",
      "Test Accuracy:0.898000\n",
      "EPOCH:356, LOSS:0.869929\n",
      "Test Accuracy:0.895400\n",
      "EPOCH:357, LOSS:0.952055\n",
      "Test Accuracy:0.897400\n",
      "EPOCH:358, LOSS:1.012760\n",
      "Test Accuracy:0.907500\n",
      "EPOCH:359, LOSS:1.040627\n",
      "Test Accuracy:0.872700\n",
      "EPOCH:360, LOSS:1.002591\n",
      "Test Accuracy:0.904800\n",
      "EPOCH:361, LOSS:0.797497\n",
      "Test Accuracy:0.891500\n",
      "EPOCH:362, LOSS:0.808933\n",
      "Test Accuracy:0.892000\n",
      "EPOCH:363, LOSS:0.854914\n",
      "Test Accuracy:0.898700\n",
      "EPOCH:364, LOSS:0.823813\n",
      "Test Accuracy:0.899600\n",
      "EPOCH:365, LOSS:0.740900\n",
      "Test Accuracy:0.908400\n",
      "EPOCH:366, LOSS:0.793897\n",
      "Test Accuracy:0.906000\n",
      "EPOCH:367, LOSS:0.741289\n",
      "Test Accuracy:0.914200\n",
      "EPOCH:368, LOSS:0.844962\n",
      "Test Accuracy:0.899100\n",
      "EPOCH:369, LOSS:0.858417\n",
      "Test Accuracy:0.898000\n",
      "EPOCH:370, LOSS:0.969867\n",
      "Test Accuracy:0.895200\n",
      "EPOCH:371, LOSS:0.926020\n",
      "Test Accuracy:0.910000\n",
      "EPOCH:372, LOSS:0.789752\n",
      "Test Accuracy:0.879300\n",
      "EPOCH:373, LOSS:0.805033\n",
      "Test Accuracy:0.898700\n",
      "EPOCH:374, LOSS:0.698197\n",
      "Test Accuracy:0.878300\n",
      "EPOCH:375, LOSS:0.818846\n",
      "Test Accuracy:0.899900\n",
      "EPOCH:376, LOSS:0.692974\n",
      "Test Accuracy:0.913400\n",
      "EPOCH:377, LOSS:0.650660\n",
      "Test Accuracy:0.916900\n",
      "EPOCH:378, LOSS:0.704320\n",
      "Test Accuracy:0.906300\n",
      "EPOCH:379, LOSS:0.798615\n",
      "Test Accuracy:0.905100\n",
      "EPOCH:380, LOSS:0.675685\n",
      "Test Accuracy:0.903200\n",
      "EPOCH:381, LOSS:0.743924\n",
      "Test Accuracy:0.893100\n",
      "EPOCH:382, LOSS:0.730370\n",
      "Test Accuracy:0.909500\n",
      "EPOCH:383, LOSS:0.734294\n",
      "Test Accuracy:0.907900\n",
      "EPOCH:384, LOSS:0.766971\n",
      "Test Accuracy:0.903500\n",
      "EPOCH:385, LOSS:0.881552\n",
      "Test Accuracy:0.888300\n",
      "EPOCH:386, LOSS:0.714256\n",
      "Test Accuracy:0.891200\n",
      "EPOCH:387, LOSS:0.707431\n",
      "Test Accuracy:0.902800\n",
      "EPOCH:388, LOSS:0.648900\n",
      "Test Accuracy:0.911000\n",
      "EPOCH:389, LOSS:0.751803\n",
      "Test Accuracy:0.890400\n",
      "EPOCH:390, LOSS:0.713094\n",
      "Test Accuracy:0.911400\n",
      "EPOCH:391, LOSS:0.731257\n",
      "Test Accuracy:0.914800\n",
      "EPOCH:392, LOSS:0.591645\n",
      "Test Accuracy:0.905300\n",
      "EPOCH:393, LOSS:0.730630\n",
      "Test Accuracy:0.894800\n",
      "EPOCH:394, LOSS:0.785576\n",
      "Test Accuracy:0.907900\n",
      "EPOCH:395, LOSS:0.761600\n",
      "Test Accuracy:0.906200\n",
      "EPOCH:396, LOSS:0.732462\n",
      "Test Accuracy:0.902800\n",
      "EPOCH:397, LOSS:0.712654\n",
      "Test Accuracy:0.891200\n",
      "EPOCH:398, LOSS:0.668581\n",
      "Test Accuracy:0.906300\n",
      "EPOCH:399, LOSS:0.570333\n",
      "Test Accuracy:0.913200\n",
      "EPOCH:400, LOSS:0.576349\n",
      "Test Accuracy:0.887500\n"
     ]
    }
   ],
   "source": [
    "path_current = os.getcwd()\n",
    "path_ckpt = os.path.join(path_current, 'cache/models/teacher/')\n",
    "path_loss = os.path.join(path_current,'cache/experimental_data/')\n",
    "path_cifar = \"/home/ubuntu/datasets/\"\n",
    "train_teacher = TeacherTrainer(path_ckpt, path_loss, path_cifar, bs=2048, num_epochsaving=150)\n",
    "train_teacher.train(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/YZP/gitee/paper-reading/DeepInversion/cache/models/teacher/teacher\n",
      "Using trained teacher network...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "randn() received an invalid combination of arguments - got (tuple, dtype=torch.dtype, required_grad=bool, device=str), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-cc0bda4a4734>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# net = ResNet34().cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# net.load_state_dict(ckpt['net'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_ckpt_teacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_save_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-eb0324defe82>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_ckpt_teacher, path_inputs_saving)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequired_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: randn() received an invalid combination of arguments - got (tuple, dtype=torch.dtype, required_grad=bool, device=str), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "print('start...')\n",
    "path_current = os.getcwd()\n",
    "\n",
    "path_ckpt_teacher = os.path.join(path_current, 'paper-reading/DeepInversion/cache/models/teacher/teacher')\n",
    "print(path_ckpt_teacher)\n",
    "path_save_img = os.path.join(path_current, 'paper-reading/DeepInversion/cache/best_img.jpg')\n",
    "\n",
    "# ckpt = torch.load(path_ckpt_teacher)\n",
    "# net = ResNet34().cuda()\n",
    "# net.load_state_dict(ckpt['net'])\n",
    "trainer = InputTrainer('/home/ubuntu/YZP/gitee/paper-reading/DeepInversion/cache/models/teacher/teacher', path_save_img)\n",
    "trainer.train(1000)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b616bc69f5e56e869b6afa9b75bee44fb0b9cfffce48900ded2d9beddfe2e77a"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('torch-1.7': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
